---
title: "ECO 935 Homework 2: Ahmed Almezail"
output: md_document
---

## Problem 1: visualization

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(esquisse)
library(RColorBrewer)
library(modelr)
library(rsample)
library(mosaic)
library(fastDummies)
library(caret)
library(foreach)
library(parallel)
library(gamlr)
library(knitr)
library(parallel)
library(ROCR)
library(reshape2)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
capmetro_UT = read.csv('../data/capmetro_UT.csv')

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Recode the categorical variables in sensible, rather than alphabetical, order
capmetro_UT = mutate(capmetro_UT,
               day_of_week = factor(day_of_week,
                 levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
               month = factor(month,
                 levels=c("Sep", "Oct","Nov")))
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

 Avg_capmetro_UT = capmetro_UT %>%
  group_by(hour_of_day, day_of_week, month) %>%
  summarise_at(vars(boarding), list("Average Boarding per hour" = mean)) %>%
  data_frame()

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(Avg_capmetro_UT) +
 aes(x = hour_of_day, y = `Average Boarding per hour`, colour = month, group = month) +
 geom_line(size = 0.5) +
 scale_color_viridis_d(option = "cividis", direction = 1) +
 labs(x = "Hour of Day", 
 y = "Average Boardings", title = "Average Boardings by the Hour", subtitle = "The average riders on Capital Metro at Austin", 
 caption = "We can observe that most of the peak hour is between 3:00 pm and 4:30 pm 
 at all the weekdays on the3 different months. However, we can notice that it is 
 almost a flat line in the weekend with a very low boarding riders to the bus network. 
 That's expected since huge proportion of the riders are UT students, and there are 
 no classes on the weekends. An interesting observation is how the average boarding
 riders on Monday in September is less than on the other 2 months. 
 That's because of the Labor Day holiday which is on the first Monday of every September. 
 So that's is reflected on the graph since no classes on that day and many employees have holidays as well. 
 A similar case goes with Wednesday, Thursday, and Friday in November. 
 We can notice how lower it is comparing with the average boarding on the other weekdays in November. 
 That's simply because of the Thanksgiving Holiday which there are no classes as well on those days. 
 Those holidays affect the average lines and makes it lower than its peers.") +
 theme_minimal() +
 facet_wrap(vars(day_of_week)) +
    theme(plot.caption = element_text(hjust = 0.5, face= "italic"),
        plot.title = element_text(hjust=0.5), 
        plot.subtitle =element_text(hjust=0.5))





```






```{r, echo=FALSE, message=FALSE, warning=FALSE}


hours = c(
  `6` = "6:00 AM",
  `7` = "7:00 AM",
  `8` = "8:00 AM",
  `9` = "9:00 AM",
  `10` = "10:00 AM",
  `11` = "11:00 AM",
  `12` = "12:00 PM",
  `13` = "1:00 PM",
  `14` = "2:00 PM",
  `15` = "3:00 PM",
  `16` = "4:00 PM",
  `17` = "5:00 PM",
  `18` = "6:00 PM",
  `19` = "7:00 PM",
  `20` = "8:00 PM",
  `21` = "9:00 PM"
)

ggplot(capmetro_UT) +
 aes(x = temperature, y = boarding, colour = weekend, group = month) +
 geom_point(size = 0.5) +
 scale_color_viridis_d(option = "cividis", direction = 1) +
 labs(x = "Temperature", 
 y = "Boardings", title = "Number of Boardings vs Temperature", subtitle = "The Number of boardings for every 15 min on Capital Metro at Austin", 
 caption = "Based on the graph above, it is clear that riders are less on the weekend than on weekdays 
 as well as less on the first and last operating hours. In addition, we could also notice how temperature 
 is not a major player in affecting the numbers of riders on the bus. 
 In the peak hours we could notice a small effect based on the graphs showing that students tend to 
 use the bus more often when temperature is increasing.") +
 theme_minimal() +
 facet_wrap(vars(hour_of_day), labeller = as_labeller(hours)) +  
  theme(plot.caption = element_text(hjust = 0.5, face= "italic"),
        plot.title = element_text(hjust=0.5), 
        plot.subtitle =element_text(hjust=0.5))

```





## Problem 2: Saratoga house prices

Let's have a look into the data...

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data(SaratogaHouses)
glimpse(SaratogaHouses)

```

### Now we want to build a better model than the one in the class.


```{r}

set.seed(430)
saratoga_split = createDataPartition(SaratogaHouses$price, p = 0.75, list = FALSE)
saratoga_train = SaratogaHouses[saratoga_split, ]
saratoga_test = SaratogaHouses[-saratoga_split, ]

lm_class = train(
  price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train,
  method = "lm", 
  trControl = trainControl(method = "cv", 
               number = 10, verboseIter = TRUE ))


lm_me = train(
  price ~ . *(landValue) - (rooms -centralAir - fuel - fireplaces- pctCollege - sewer - waterfront - landValue - newConstruction), data=saratoga_train,
  method = "lm", 
  trControl = trainControl(method = "cv", 
               number = 10, verboseIter = TRUE ))



lm_class$finalModel
lm_me$finalModel

lm_class$results
lm_me$results




```


Based on the RMSE results of the training data, my model outperformed the one in the class.


### Now lets use KNN model...

```{r, echo=FALSE, message=FALSE, warning=FALSE}

##KNN Model




Xtrain = model.matrix(~ age + livingArea  + bedrooms + bathrooms + landValue - 1, data=saratoga_train) 
Xtest = model.matrix(~ age + livingArea  + bedrooms + bathrooms + landValue - 1, data=saratoga_test)

# training and testing set responses
ytrain = saratoga_train$price 
ytest = saratoga_test$price


# now rescale:
scale_train = apply(Xtrain, 2, sd) # calculate std dev for each column 
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale = scale_train) # use the training set scales!
Xtilde_test=data.frame(Xtilde_test)%>%
  mutate(price=c(ytest))
Xtilde_train=data.frame(Xtilde_train)%>%
  mutate(price=c(ytrain))


rmse_saratoga=foreach(i=1:100, .combine='c') %do% {
  knn_model_saratoga= knnreg(price ~  age +  livingArea + bedrooms + bathrooms + landValue - 1,
                             data=Xtilde_train, k=i)
 knn_best = modelr::rmse(knn_model_saratoga,Xtilde_test)
} 



plot(rmse_saratoga)


```



```{r echo=FALSE, message=FALSE, warning=FALSE}


rmse1_tax = rmse(lm_class, saratoga_test)
rmse2_tax = rmse(lm_me, saratoga_test)
rmse3_tax = knn_best


models_tax_summary = data.frame(
Model1_tax_rmse = rmse1_tax,
Model2_tax_rmse = rmse2_tax,
model3_tax_rmse = rmse3_tax)

models_tax_summary

```


Based on the testing data, my linear model outperformed the other two since it has the lowest rmse value based on the testing set.







### Tax Report Price Predictions

Based on our analysis, we recommend to use my linear model predictions since it has the lowest RMSE. In this model we emphasize on the most effective factors on property prices. We observe that prices depend more on these factors:-

1- age of the property
2- living area in square feet
3- number of bedrooms
4- number of bathrooms
5- heating systems
6- lot size

and we believe all these variables depend on the land value, so we took that under consideration. We came up with this model since the first variables are the most common factors on pricing strategy, and we considers the heating systems since it gets very cold in winter in New York. The lot size is another variable that we think will add more predictive power to our model. However, we thought about how the land value would play crucial role in pricing, so we included that too in our predictive model in a how each effect of those 6 variables would change depending on the value of the property land.

So, we can predict the price of a property house if we know only these data. We believe these factors have a good price estimation power. Adding the other factors might affect our predictive power for the model. However, it is very important to mention that our model have an average deviation from the market price by $57,763.44. So you might take that under consideration when taxing the property owners. 


So, your tax authority could use these data to know whether the market price is overvalued or undervalued, and use all these information to create the best pricing estimation for taxing purposes.






## Problem 3: Classification and retrospective sampling

```{r, echo=FALSE, message=FALSE, warning=FALSE}

german_credit = read.csv('../data/german_credit.csv')

```




```{r, echo=FALSE, message=FALSE, warning=FALSE}

xtabs(~Default + history, data = german_credit)

prop_def = xtabs(~Default + history, data = german_credit) %>%
  prop.table(margin = 2) %>%
  data.frame()


ggplot(prop_def) +
 aes(x = Default, fill = history, colour = history, weight = Freq) +
 geom_bar() +
 scale_fill_manual(values = c(good = "#003C30", 
poor = "#F2DA04", terrible = "#C43D0E")) +
 scale_color_manual(values = c(good = "#003C30", poor = "#F2DA04", 
terrible = "#C43D0E")) +
 labs(title = "Probability of Default Conditional on Borrowers' History") +
 theme_minimal() +
 facet_wrap(vars(history))


```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Recall: the dot (.) says "use all variables not otherwise named"
logit_def = glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data=german_credit, family='binomial')

coef(logit_def) %>% round(2)


def_history = german_credit %>%
  mutate(default_pred = predict(logit_def, german_credit)) %>%
  mutate(Default_Odds = exp(default_pred))
  
  def_history = def_history %>%
  select(history, Default, default_pred, Default_Odds)
  data.frame()
  
  

ggplot(def_history) +
 aes(x = history, y = Default_Odds, colour = history) +
 geom_jitter(size = 1.5) +
 scale_color_manual(values = c(good = "#003C30", poor = "#DBC711", terrible = "#C02B05")) +
 labs(x = "History", 
 y = "Odds", title = "Expected Odds of Default based on Borrowers' History") +
 theme_minimal()



```






The model suggests that clients with good credit history have higher probability of a loan default than an average client with terrible or poor credit histories. This predictive model can't be rely on since it does not reflect the reality. So we cannot rely on such a model to predict the default odds of a client based on the history. Also, the bank should adjust the methodology of the case control design. I will state below the specific concerns with the model. 

Does the bank offer the same probability of lending a loan regardless of client's credit history? Because it might be the fact that the bank have already dodged a dozen of bullets when they rejected loan proposals from terrible or poor clients. So if this hypothesis is true, we expect to have more default odds for a client with a good credit history since they have less due diligence analysis applied on them. In this case, when we perform the comparison between set of defaulted clients and not-defaulted ones with the same credit history, that would be biased analysis. The conclusion that we can take from this logistic model is that the bank should do more due diligence on clients with good credit history.

or should they? the data needs more diagnostic...

The sample taken is biased since it is not random. Clustering the data based on credit history would give us a biased sample that we could not rely on. When the bank took a random sample of loans and compared that with the set of close matched loans, that means if you are a client with a good credit history, and you have defaulted, then you have higher probability of default if I compare you with clients with the same history. That's because on average, most of clients with good score don't default, so we should not compare them with the same history score. So it should be random sample with random clustering in order to have a reliable data.

In short, just improve the methodology of case design in order to have unbiased model.






## Problem 4: Children and hotel reservations



```{r, echo=FALSE, message=FALSE, warning=FALSE}

hotels_dev = read.csv('../data/hotels_dev.csv')
hotels_val = read.csv('../data/hotels_val.csv')

```

### Model building

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Split into training and testing sets

set.seed(10)
hotels_dev_split = createDataPartition(hotels_dev$children, p = 0.75, list = FALSE)
hotels_dev_train = hotels_dev[hotels_dev_split, ]
hotels_dev_test = hotels_dev[-hotels_dev_split, ]



logit_dev1 = train(
 children ~ market_segment + adults + customer_type + is_repeated_guest -1, data=hotels_dev_train,
  method = "glm", 
  trControl = trainControl(method = "cv", 
               number = 20), 
               family = "binomial")

logit_dev2 = train(
 children ~ . - arrival_date -1, data=hotels_dev_train,
  method = "glm", 
  trControl = trainControl(method = "cv", 
               number = 20), 
               family = "binomial")

logit_dev3 = train(
children ~ . - arrival_date + average_daily_rate*adults -1 , data=hotels_dev_train,
  method = "glm", 
  trControl = trainControl(method = "cv", 
               number = 20), 
               family = "binomial") 


logit_dev1$finalModel
logit_dev2$finalModel
logit_dev3$finalModel

logit_dev1$results
logit_dev2$results
logit_dev3$results






```


Based on the AIC and the RMSE, the third model is the best fitting model among all the three.


```{r echo=FALSE, message=FALSE, warning=FALSE}


rmse1 = rmse(logit_dev1, hotels_dev_test)
rmse2 = rmse(logit_dev2, hotels_dev_test)
rmse3 = rmse(logit_dev3, hotels_dev_test)


models_summary = data.frame(
Model1_rmse = rmse1,
Model2_rmse = rmse2,
model3_rmse = rmse3)

models_summary

```

Based on the results of the models, the 3rd model outperformed the other 2 other models, and have better rmse than the training data which implies good fitting and reliable prediction power.



### Model validation Step 1


```{r echo=FALSE, message=FALSE, warning=FALSE}




glm_val = glm(children ~ . - arrival_date + average_daily_rate*adults -1, data=hotels_val)






glm_val_pred = predict(glm_val, hotels_val)
  

val_pred_test1 = ifelse(glm_val_pred > 0.5, 1, 0)
val_pred_test2 = ifelse(glm_val_pred > 0.8, 1, 0)

confusion1 = table(children = hotels_val$children, pred_children = val_pred_test1)
confusion2 = table(children = hotels_val$children, pred_children = val_pred_test2)




roc_pred = prediction(predictions = glm_val_pred  , labels = hotels_val$children)
roc_perform = performance(roc_pred , "tpr" , "fpr")
plot(roc_perform)



```

Based on the ROC curve, the best value would be the furthest value in the northwest.


### Model validation Step 2



```{r echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}

# allocate to folds

n = nrow(hotels_val)
K = 20
fold_id = rep_len(1:K, n)  # repeats 1:K over and over again
fold_id = sample(fold_id, replace=FALSE) # permute the order randomly
hotels_val_fold = cbind(hotels_val, fold_id)

children_predictions = c()
children_real = c()
seq = seq(1:20)

for(i in 1:K) {
hotels_val_fold_i = hotels_val_fold %>% 
  filter(fold_id == i)
val_model_pred = predict(glm_val, newdata = hotels_val_fold_i)
children_pred_fold = sum(val_model_pred)
children_predictions = c(children_predictions, children_pred_fold)
children_fold = sum(hotels_val_fold_i$children)
children_real = c(children_real, children_fold)
}

error_sq = (children_real - children_predictions)^2
rmse_final = sqrt(sum(error_sq)/20)
fold_performance = data.frame(cbind(seq, children_predictions, children_real))


fold_performance_mtx <- melt(fold_performance[,c('seq','children_predictions','children_real')],id.vars = 1)

ggplot(fold_performance_mtx,aes(x = seq,y = value)) + 
    geom_bar(aes(fill = variable),stat = "identity",position = "dodge") + 
    scale_y_log10() +
 labs(title= "Number of Children per fold", subtitle = "True number  vs Predicted number", ) +
  theme_minimal()

fold_performance_mtx_p = fold_performance_mtx %>%
  mutate(fold_performance_mtx, children_probability = value/250)

df =aggregate(fold_performance_mtx_p$children_probability, list(fold_performance_mtx_p$variable), FUN=mean) %>%
  data.frame()

df <- setNames(df, c("Pred vs Real","Avg Prob"))

fold_performance_mtx_p
df

models_val = data.frame(
Model1_rmse = rmse1,
Model2_rmse = rmse2,
model3_rmse = rmse3,
model_val_rmse = rmse_final)

models_val

```

This plot shows the predicted number of children per fold in red, and the true number of children per fold in blue. We can notice how the predictions deviates from the true value by an average of 3.4 children across all the folds. The rmse in the validation model is a bit higher than in the models built from the original data. It is expected since the model was not tailored based on the validation data, but it gave us quite good estimation.

Furthermore, the true and predicted number of children have the same probability of a booking that have a kid which it is equal to 8% probability. In conclustion, I believe this model have good predictive power, and could be relied on to estimate the number of children in a booking on a single busy weekend.




